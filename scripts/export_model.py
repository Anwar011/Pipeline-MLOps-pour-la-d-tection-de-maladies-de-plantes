#!/usr/bin/env python3
"""
Script d'export du mod√®le en ONNX et TorchScript.
Conforme au cahier des charges: Registry et packaging (ONNX/TorchScript).
"""

import argparse
import logging
import os
import sys
from pathlib import Path

import mlflow
import torch
import yaml

# Ajouter le r√©pertoire src au path
sys.path.append(str(Path(__file__).parent.parent / "src"))

from models import create_model

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def export_model(config_path: str = "config.yaml"):
    """
    Exporte le mod√®le en diff√©rents formats pour la production.
    
    Formats d'export:
    - PyTorch checkpoint (.ckpt)
    - ONNX (.onnx) - pour l'inf√©rence optimis√©e
    - TorchScript (.pt) - pour le d√©ploiement
    """
    # Charger la configuration
    with open(config_path, "r") as f:
        config = yaml.safe_load(f)
    
    # Cr√©er le dossier de production
    production_dir = Path("models/production")
    production_dir.mkdir(parents=True, exist_ok=True)
    
    # Trouver le meilleur checkpoint
    checkpoint_dir = Path(config["training"]["checkpoint_path"])
    checkpoints = list(checkpoint_dir.glob("*.ckpt"))
    
    if not checkpoints:
        logger.error("‚ùå Aucun checkpoint trouv√© pour l'export!")
        logger.info("üí° Entra√Ænez d'abord un mod√®le avec: python src/train.py")
        return None
    
    # Prendre le checkpoint le plus r√©cent (ou le meilleur bas√© sur le nom)
    best_checkpoint = sorted(checkpoints, key=lambda x: x.stat().st_mtime)[-1]
    logger.info(f"üìÇ Chargement du checkpoint: {best_checkpoint}")
    
    # Charger le mod√®le
    model = create_model("cnn", config_path)
    checkpoint = torch.load(best_checkpoint, map_location="cpu")
    model.load_state_dict(checkpoint["state_dict"])
    model.eval()
    
    # 1. Export PyTorch checkpoint pour production
    production_ckpt = production_dir / "model.ckpt"
    torch.save({
        "state_dict": model.state_dict(),
        "config": config,
        "architecture": config["model"]["architecture"],
        "num_classes": config["model"]["num_classes"]
    }, production_ckpt)
    logger.info(f"‚úÖ Checkpoint export√©: {production_ckpt}")
    
    # 2. Export ONNX
    export_onnx(model, config, production_dir)
    
    # 3. Export TorchScript
    export_torchscript(model, config, production_dir)
    
    # 4. Enregistrer dans MLflow Model Registry
    register_model_mlflow(model, config)
    
    logger.info("‚úÖ Export du mod√®le termin√©!")
    
    return {
        "checkpoint": str(production_ckpt),
        "onnx": str(production_dir / "model.onnx"),
        "torchscript": str(production_dir / "model.pt")
    }


def export_onnx(model, config, output_dir):
    """Exporte le mod√®le au format ONNX."""
    try:
        onnx_path = output_dir / "model.onnx"
        
        # Cr√©er un input dummy
        image_size = config["data"]["image_size"]
        dummy_input = torch.randn(1, 3, image_size[0], image_size[1])
        
        # Export ONNX
        torch.onnx.export(
            model,
            dummy_input,
            onnx_path,
            export_params=True,
            opset_version=12,
            do_constant_folding=True,
            input_names=["input"],
            output_names=["output"],
            dynamic_axes={
                "input": {0: "batch_size"},
                "output": {0: "batch_size"}
            }
        )
        
        logger.info(f"‚úÖ Mod√®le ONNX export√©: {onnx_path}")
        
        # V√©rifier l'export ONNX
        try:
            import onnx
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            logger.info("‚úÖ Mod√®le ONNX valid√©")
        except ImportError:
            logger.warning("‚ö†Ô∏è  onnx non install√©, validation ignor√©e")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'export ONNX: {e}")


def export_torchscript(model, config, output_dir):
    """Exporte le mod√®le au format TorchScript."""
    try:
        script_path = output_dir / "model.pt"
        
        # Cr√©er un input dummy
        image_size = config["data"]["image_size"]
        dummy_input = torch.randn(1, 3, image_size[0], image_size[1])
        
        # Export TorchScript via tracing
        traced_model = torch.jit.trace(model, dummy_input)
        traced_model.save(str(script_path))
        
        logger.info(f"‚úÖ Mod√®le TorchScript export√©: {script_path}")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'export TorchScript: {e}")


def register_model_mlflow(model, config):
    """Enregistre le mod√®le dans MLflow Model Registry."""
    try:
        mlflow_config = config["mlflow"]
        mlflow.set_tracking_uri(mlflow_config["tracking_uri"])
        
        # Cr√©er un run pour l'enregistrement
        with mlflow.start_run(run_name="model_registration"):
            # Logger le mod√®le
            mlflow.pytorch.log_model(
                model,
                artifact_path="model",
                registered_model_name="plant_disease_model"
            )
            
            # Logger les param√®tres
            mlflow.log_params({
                "architecture": config["model"]["architecture"],
                "num_classes": config["model"]["num_classes"],
                "image_size": str(config["data"]["image_size"])
            })
        
        logger.info("‚úÖ Mod√®le enregistr√© dans MLflow Model Registry")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Impossible d'enregistrer dans MLflow: {e}")
        logger.info("üí° Assurez-vous que le serveur MLflow est d√©marr√©")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Exporter le mod√®le")
    parser.add_argument("--config", type=str, default="config.yaml")
    args = parser.parse_args()
    
    export_model(args.config)
