#!/bin/bash

# ============================================
# Script de Tests Automatis√©s
# D√©tection de Maladies de Plantes
# ============================================

set -e  # Arr√™ter le script en cas d'erreur

# Couleurs pour la sortie
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Fonction de logging
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

error() {
    echo -e "${RED}[ERROR] $1${NC}" >&2
}

info() {
    echo -e "${BLUE}[INFO] $1${NC}"
}

warning() {
    echo -e "${YELLOW}[WARNING] $1${NC}"
}

# Configuration
API_URL=${API_URL:-http://localhost:8000}
TEST_IMAGE=${TEST_IMAGE:-test_plant.jpg}
COVERAGE_REPORT=${COVERAGE_REPORT:-htmlcov}

# V√©rification des pr√©requis
check_prerequisites() {
    log "V√©rification des pr√©requis pour les tests..."

    # V√©rifier Python
    if ! command -v python &> /dev/null; then
        error "Python n'est pas install√©"
        exit 1
    fi

    # V√©rifier pip
    if ! command -v pip &> /dev/null; then
        error "pip n'est pas install√©"
        exit 1
    fi

    # V√©rifier pytest
    if ! python -c "import pytest" &> /dev/null; then
        warning "pytest non install√©. Installation..."
        pip install pytest pytest-cov pytest-asyncio httpx
    fi

    log "Pr√©requis v√©rifi√©s ‚úì"
}

# Tests unitaires
run_unit_tests() {
    log "Lancement des tests unitaires..."

    # Installer les d√©pendances de test
    pip install -r requirements-train.txt  # Pour les tests d'entra√Ænement

    # Lancer les tests avec couverture
    pytest tests/ -v \
        --cov=src \
        --cov-report=html:$COVERAGE_REPORT \
        --cov-report=term-missing \
        --cov-fail-under=80

    log "Tests unitaires termin√©s ‚úì"
}

# Tests d'int√©gration API
run_api_tests() {
    log "Lancement des tests d'int√©gration API..."

    # V√©rifier que l'API est accessible
    if ! curl -s $API_URL/health > /dev/null; then
        error "API non accessible sur $API_URL"
        info "Lancez d'abord: bash scripts/run_api.sh dev"
        exit 1
    fi

    # Test 1: Health check
    info "Test 1: Health check"
    response=$(curl -s $API_URL/health)
    if echo "$response" | grep -q "healthy"; then
        log "‚úì Health check r√©ussi"
    else
        error "‚úó Health check √©chou√©: $response"
        return 1
    fi

    # Test 2: Endpoint racine
    info "Test 2: Endpoint racine"
    response=$(curl -s $API_URL/)
    if echo "$response" | grep -q "Plant Disease Detection API"; then
        log "‚úì Endpoint racine r√©ussi"
    else
        error "‚úó Endpoint racine √©chou√©"
        return 1
    fi

    # Test 3: Classes support√©es
    info "Test 3: Classes support√©es"
    response=$(curl -s $API_URL/classes)
    if echo "$response" | grep -q "classes"; then
        log "‚úì Classes r√©cup√©r√©es"
    else
        error "‚úó R√©cup√©ration des classes √©chou√©e"
        return 1
    fi

    # Test 4: Informations mod√®le
    info "Test 4: Informations mod√®le"
    response=$(curl -s $API_URL/model/info)
    if echo "$response" | grep -q "model"; then
        log "‚úì Informations mod√®le r√©cup√©r√©es"
    else
        error "‚úó R√©cup√©ration des informations mod√®le √©chou√©e"
        return 1
    fi

    # Test 5: Pr√©diction (si image de test existe)
    if [ -f "$TEST_IMAGE" ]; then
        info "Test 5: Pr√©diction sur image de test"
        response=$(curl -s -X POST \
            -F "file=@$TEST_IMAGE" \
            $API_URL/predict)

        if echo "$response" | grep -q "prediction"; then
            log "‚úì Pr√©diction r√©ussie"
            # Afficher la pr√©diction
            echo "$response" | python -m json.tool | head -20
        else
            error "‚úó Pr√©diction √©chou√©e: $response"
            return 1
        fi
    else
        warning "Image de test non trouv√©e: $TEST_IMAGE"
        info "Cr√©ation d'une image de test factice..."
        # Cr√©er une image de test simple (1x1 pixel noir)
        python -c "
import numpy as np
from PIL import Image
img = Image.fromarray(np.zeros((32, 32, 3), dtype=np.uint8))
img.save('$TEST_IMAGE')
print('Image de test cr√©√©e')
        "
        log "‚úì Image de test cr√©√©e"
    fi

    log "Tests d'int√©gration API termin√©s ‚úì"
}

# Tests de performance
run_performance_tests() {
    log "Lancement des tests de performance..."

    # Test de charge simple
    info "Test de charge: 10 requ√™tes simultan√©es"

    # Cr√©er un script de test de charge simple
    cat > /tmp/load_test.py << 'EOF'
import asyncio
import aiohttp
import time
import statistics

async def test_request(session, url):
    start_time = time.time()
    try:
        async with session.get(url) as response:
            await response.text()
            return time.time() - start_time
    except Exception as e:
        print(f"Erreur: {e}")
        return None

async def load_test(url, num_requests=10):
    async with aiohttp.ClientSession() as session:
        tasks = [test_request(session, url) for _ in range(num_requests)]
        results = await asyncio.gather(*tasks)

        # Filtrer les r√©sultats valides
        valid_results = [r for r in results if r is not None]

        if valid_results:
            avg_time = statistics.mean(valid_results)
            min_time = min(valid_results)
            max_time = max(valid_results)
            print(".2f"            print(".2f"            print(".2f"            print(f"Requ√™tes r√©ussies: {len(valid_results)}/{num_requests}")

            # V√©rifier les contraintes (< 2s en moyenne)
            if avg_time < 2.0:
                print("‚úì Contrainte de performance respect√©e (< 2s)")
            else:
                print("‚úó Contrainte de performance non respect√©e")
        else:
            print("‚úó Aucune requ√™te r√©ussie")

asyncio.run(load_test("http://localhost:8000/health", 10))
EOF

    python /tmp/load_test.py

    log "Tests de performance termin√©s ‚úì"
}

# Tests de s√©curit√©
run_security_tests() {
    log "Lancement des tests de s√©curit√©..."

    # Test 1: Taille de fichier limit√©e
    info "Test 1: Limitation de taille de fichier"

    # Cr√©er un gros fichier de test
    dd if=/dev/zero of=/tmp/large_file.jpg bs=1M count=10 2>/dev/null

    response=$(curl -s -X POST \
        -F "file=@/tmp/large_file.jpg" \
        $API_URL/predict)

    if echo "$response" | grep -q "too large"; then
        log "‚úì Limitation de taille respect√©e"
    else
        warning "Limitation de taille non test√©e ou non respect√©e"
    fi

    rm -f /tmp/large_file.jpg

    # Test 2: Type de fichier valid√©
    info "Test 2: Validation du type de fichier"

    # Cr√©er un fichier texte d√©guis√© en image
    echo "not an image" > /tmp/fake_image.jpg

    response=$(curl -s -X POST \
        -F "file=@/tmp/fake_image.jpg" \
        $API_URL/predict)

    if echo "$response" | grep -q "Invalid image"; then
        log "‚úì Validation du type de fichier respect√©e"
    else
        warning "Validation du type de fichier non test√©e ou non respect√©e"
    fi

    rm -f /tmp/fake_image.jpg

    log "Tests de s√©curit√© termin√©s ‚úì"
}

# Tests du pipeline DVC
run_pipeline_tests() {
    log "Lancement des tests du pipeline DVC..."

    # V√©rifier que DVC est configur√©
    if [ ! -d ".dvc" ]; then
        error "DVC n'est pas initialis√©"
        exit 1
    fi

    # Test de repro des √©tapes
    info "Test de reproduction des √©tapes DVC"

    # √âtape 1: prepare_data
    if dvc repro prepare_data --dry; then
        log "‚úì √âtape prepare_data valide"
    else
        error "‚úó √âtape prepare_data invalide"
        return 1
    fi

    # √âtape 2: train
    if dvc repro train --dry; then
        log "‚úì √âtape train valide"
    else
        error "‚úó √âtape train invalide"
        return 1
    fi

    # √âtape 3: evaluate
    if dvc repro evaluate --dry; then
        log "‚úì √âtape evaluate valide"
    else
        error "‚úó √âtape evaluate invalide"
        return 1
    fi

    log "Tests du pipeline DVC termin√©s ‚úì"
}

# Rapport de test
generate_report() {
    log "G√©n√©ration du rapport de test..."

    # Cr√©er un rapport HTML simple
    cat > test_report.html << EOF
<!DOCTYPE html>
<html>
<head>
    <title>Rapport de Tests - Plant Disease Detection</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .success { color: green; }
        .error { color: red; }
        .warning { color: orange; }
        h1 { color: #2E7D32; }
        h2 { color: #1976D2; }
        pre { background: #f5f5f5; padding: 10px; border-radius: 5px; }
    </style>
</head>
<body>
    <h1>üß™ Rapport de Tests - D√©tection de Maladies de Plantes</h1>
    <p><strong>Date:</strong> $(date)</p>
    <p><strong>Environnement:</strong> $(uname -a)</p>

    <h2>R√©sum√© des Tests</h2>
    <p>Tests ex√©cut√©s avec succ√®s. Voir les d√©tails ci-dessous.</p>

    <h2>Couverture de Code</h2>
    <p>Rapport disponible dans: <a href="$COVERAGE_REPORT/index.html">$COVERAGE_REPORT/index.html</a></p>

    <h2>Recommandations</h2>
    <ul>
        <li>V√©rifier r√©guli√®rement la couverture de code (> 80%)</li>
        <li>Ex√©cuter les tests avant chaque d√©ploiement</li>
        <li>Monitorer les performances en production</li>
    </ul>
</body>
</html>
EOF

    log "Rapport g√©n√©r√©: test_report.html"
}

# Fonction principale
main() {
    echo "============================================"
    echo "üß™ Tests Automatis√©s - D√©tection de Maladies"
    echo "============================================"

    # V√©rifier les arguments
    case "${1:-all}" in
        "unit")
            check_prerequisites
            run_unit_tests
            ;;
        "api")
            check_prerequisites
            run_api_tests
            ;;
        "perf")
            check_prerequisites
            run_performance_tests
            ;;
        "security")
            check_prerequisites
            run_security_tests
            ;;
        "pipeline")
            check_prerequisites
            run_pipeline_tests
            ;;
        "report")
            generate_report
            ;;
        "all")
            check_prerequisites
            run_unit_tests
            run_api_tests
            run_performance_tests
            run_security_tests
            run_pipeline_tests
            generate_report
            ;;
        *)
            error "Usage: $0 {unit|api|perf|security|pipeline|report|all}"
            echo "  unit     : Tests unitaires"
            echo "  api      : Tests d'int√©gration API"
            echo "  perf     : Tests de performance"
            echo "  security : Tests de s√©curit√©"
            echo "  pipeline : Tests du pipeline DVC"
            echo "  report   : G√©n√©rer un rapport"
            echo "  all      : Tous les tests"
            exit 1
            ;;
    esac

    log "üéâ Tests termin√©s avec succ√®s !"
}

# Gestion des signaux pour un arr√™t propre
trap 'echo -e "\n${YELLOW}Arr√™t des tests...${NC}"; exit 0' INT TERM

# Lancer la fonction principale
main "$@"