#!/usr/bin/env python3
"""
Pipeline automatis√© complet:
1. D√©tecte les changements DVC
2. Ex√©cute le pipeline DVC (prepare_data -> train -> evaluate -> export)
3. Enregistre les donn√©es et mod√®les dans MLflow
4. Reconstruit l'image Docker avec le nouveau mod√®le
5. Red√©ploie localement avec Docker Compose
"""

import argparse
import logging
import os
import subprocess
import sys
import time
from pathlib import Path
from typing import Dict, Optional

import yaml

# Ajouter le r√©pertoire src au path
sys.path.append(str(Path(__file__).parent.parent / "src"))

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class AutomatedPipeline:
    """Pipeline automatis√© MLOps."""
    
    def __init__(self, config_path: str = "config.yaml", force: bool = False):
        self.config_path = Path(config_path)
        self.project_root = Path(__file__).parent.parent
        self.force = force
        
        # Charger la configuration
        with open(self.config_path, "r") as f:
            self.config = yaml.safe_load(f)
        
        # Chemins importants
        self.dvc_yaml = self.project_root / "dvc.yaml"
        self.models_dir = self.project_root / "models"
        self.production_model = self.models_dir / "production" / "model.ckpt"
        self.docker_compose = self.project_root / "docker" / "docker-compose.yml"
        
    def check_dvc_changes(self) -> bool:
        """V√©rifier si des changements DVC sont d√©tect√©s."""
        logger.info("üîç V√©rification des changements DVC...")
        
        try:
            # Import relatif depuis le m√™me r√©pertoire
            sys.path.insert(0, str(Path(__file__).parent))
            from monitor_dvc_changes import DVCChangeMonitor
            monitor = DVCChangeMonitor(project_root=str(self.project_root))
            has_changes = monitor.has_changes()
            
            if has_changes:
                logger.info("‚úÖ Changements DVC d√©tect√©s!")
            else:
                logger.info("‚ÑπÔ∏è  Aucun changement DVC d√©tect√©")
            
            return has_changes or self.force
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erreur lors de la v√©rification DVC: {e}")
            logger.info("‚ÑπÔ∏è  Continuation avec l'option --force")
            return self.force
    
    def run_dvc_pipeline(self) -> bool:
        """Ex√©cuter le pipeline DVC complet."""
        logger.info("üöÄ Ex√©cution du pipeline DVC...")
        
        if not self.dvc_yaml.exists():
            logger.error(f"‚ùå dvc.yaml non trouv√©: {self.dvc_yaml}")
            return False
        
        try:
            # Ex√©cuter dvc repro pour ex√©cuter tout le pipeline
            logger.info("üìä Ex√©cution: dvc repro")
            result = subprocess.run(
                ["dvc", "repro"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                check=False
            )
            
            if result.returncode == 0:
                logger.info("‚úÖ Pipeline DVC ex√©cut√© avec succ√®s")
                logger.info(result.stdout)
                return True
            else:
                logger.error(f"‚ùå Erreur lors de l'ex√©cution du pipeline DVC:")
                logger.error(result.stderr)
                return False
                
        except FileNotFoundError:
            logger.error("‚ùå DVC n'est pas install√©. Installez-le avec: pip install dvc")
            return False
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'ex√©cution du pipeline DVC: {e}")
            return False
    
    def verify_mlflow_registration(self) -> bool:
        """V√©rifier que le mod√®le est enregistr√© dans MLflow."""
        logger.info("üìù V√©rification de l'enregistrement MLflow...")
        
        mlflow_config = self.config.get("mlflow", {})
        tracking_uri = mlflow_config.get("tracking_uri", "http://localhost:5000")
        
        try:
            import mlflow
            mlflow.set_tracking_uri(tracking_uri)
            
            # V√©rifier la connexion
            experiments = mlflow.search_experiments()
            logger.info(f"‚úÖ MLflow connect√©: {len(experiments)} exp√©riences trouv√©es")
            
            # V√©rifier le dernier run
            experiment_name = mlflow_config.get("experiment_name", "plant_disease_detection")
            try:
                experiment = mlflow.get_experiment_by_name(experiment_name)
                if experiment:
                    runs = mlflow.search_runs(
                        experiment_ids=[experiment.experiment_id],
                        max_results=1
                    )
                    if not runs.empty:
                        latest_run = runs.iloc[0]
                        logger.info(f"‚úÖ Dernier run MLflow: {latest_run['run_id']}")
                        logger.info(f"   M√©triques: {latest_run.get('metrics.val_acc', 'N/A')}")
                        return True
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Impossible de v√©rifier les runs: {e}")
            
            return True
        except ImportError:
            logger.warning("‚ö†Ô∏è  MLflow non disponible")
            return True  # Ne pas bloquer si MLflow n'est pas disponible
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Erreur MLflow: {e}")
            return True  # Ne pas bloquer
    
    def verify_model_production(self) -> bool:
        """V√©rifier que le mod√®le de production existe."""
        logger.info("üîç V√©rification du mod√®le de production...")
        
        if self.production_model.exists():
            size_mb = self.production_model.stat().st_size / (1024 * 1024)
            logger.info(f"‚úÖ Mod√®le de production trouv√©: {self.production_model}")
            logger.info(f"   Taille: {size_mb:.2f} MB")
            return True
        else:
            logger.error(f"‚ùå Mod√®le de production non trouv√©: {self.production_model}")
            logger.info("üí° Le mod√®le devrait √™tre cr√©√© lors de l'entra√Ænement")
            return False
    
    def build_docker_image(self) -> bool:
        """Reconstruire l'image Docker avec le nouveau mod√®le."""
        logger.info("üê≥ Reconstruction de l'image Docker...")
        
        dockerfile = self.project_root / "docker" / "Dockerfile.inference"
        if not dockerfile.exists():
            logger.error(f"‚ùå Dockerfile non trouv√©: {dockerfile}")
            return False
        
        image_name = self.config.get("docker", {}).get("image_name", "plant-disease-mlops")
        image_tag = self.config.get("docker", {}).get("tag", "latest")
        full_image_name = f"{image_name}:{image_tag}"
        
        try:
            # V√©rifier que le mod√®le existe avant de construire
            if not self.verify_model_production():
                logger.error("‚ùå Impossible de construire l'image: mod√®le manquant")
                return False
            
            logger.info(f"üî® Construction de l'image: {full_image_name}")
            
            result = subprocess.run(
                [
                    "docker", "build",
                    "-f", str(dockerfile),
                    "-t", full_image_name,
                    str(self.project_root)
                ],
                capture_output=True,
                text=True,
                check=False
            )
            
            if result.returncode == 0:
                logger.info("‚úÖ Image Docker construite avec succ√®s")
                return True
            else:
                logger.error(f"‚ùå Erreur lors de la construction de l'image:")
                logger.error(result.stderr)
                return False
                
        except FileNotFoundError:
            logger.error("‚ùå Docker n'est pas install√© ou non disponible")
            return False
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la construction Docker: {e}")
            return False
    
    def deploy_locally(self) -> bool:
        """Red√©ployer localement avec Docker Compose."""
        logger.info("üöÄ D√©ploiement local avec Docker Compose...")
        
        if not self.docker_compose.exists():
            logger.error(f"‚ùå docker-compose.yml non trouv√©: {self.docker_compose}")
            return False
        
        try:
            # Arr√™ter les services existants
            logger.info("üõë Arr√™t des services existants...")
            subprocess.run(
                ["docker-compose", "-f", str(self.docker_compose), "down"],
                cwd=self.project_root,
                capture_output=True,
                check=False
            )
            
            # D√©marrer les services
            logger.info("‚ñ∂Ô∏è  D√©marrage des services...")
            result = subprocess.run(
                ["docker-compose", "-f", str(self.docker_compose), "up", "-d"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                check=False
            )
            
            if result.returncode == 0:
                logger.info("‚úÖ Services d√©ploy√©s avec succ√®s")
                logger.info("üìä V√©rification du statut...")
                
                # Attendre un peu pour que les services d√©marrent
                time.sleep(5)
                
                # V√©rifier le statut
                status_result = subprocess.run(
                    ["docker-compose", "-f", str(self.docker_compose), "ps"],
                    cwd=self.project_root,
                    capture_output=True,
                    text=True,
                    check=False
                )
                logger.info(status_result.stdout)
                
                return True
            else:
                logger.error(f"‚ùå Erreur lors du d√©ploiement:")
                logger.error(result.stderr)
                return False
                
        except FileNotFoundError:
            logger.error("‚ùå docker-compose n'est pas install√©")
            return False
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du d√©ploiement: {e}")
            return False
    
    def run_full_pipeline(self) -> bool:
        """Ex√©cuter le pipeline complet."""
        logger.info("=" * 60)
        logger.info("üöÄ D√âMARRAGE DU PIPELINE AUTOMATIS√â MLOPS")
        logger.info("=" * 60)
        
        steps = [
            ("V√©rification DVC", self.check_dvc_changes),
            ("Pipeline DVC", self.run_dvc_pipeline),
            ("V√©rification MLflow", self.verify_mlflow_registration),
            ("V√©rification Mod√®le", self.verify_model_production),
            ("Construction Docker", self.build_docker_image),
            ("D√©ploiement Local", self.deploy_locally),
        ]
        
        results = {}
        
        for step_name, step_func in steps:
            logger.info("")
            logger.info(f"üìå √âtape: {step_name}")
            logger.info("-" * 60)
            
            try:
                start_time = time.time()
                success = step_func()
                elapsed = time.time() - start_time
                
                results[step_name] = {
                    "success": success,
                    "elapsed": elapsed
                }
                
                if not success:
                    logger.error(f"‚ùå √âchec de l'√©tape: {step_name}")
                    logger.error("üõë Arr√™t du pipeline")
                    break
                else:
                    logger.info(f"‚úÖ √âtape termin√©e en {elapsed:.2f}s")
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur dans l'√©tape {step_name}: {e}")
                results[step_name] = {
                    "success": False,
                    "error": str(e)
                }
                break
        
        # R√©sum√©
        logger.info("")
        logger.info("=" * 60)
        logger.info("üìä R√âSUM√â DU PIPELINE")
        logger.info("=" * 60)
        
        for step_name, result in results.items():
            status = "‚úÖ" if result.get("success") else "‚ùå"
            elapsed = result.get("elapsed", 0)
            logger.info(f"{status} {step_name}: {elapsed:.2f}s")
        
        all_success = all(r.get("success", False) for r in results.values())
        
        if all_success:
            logger.info("")
            logger.info("üéâ PIPELINE TERMIN√â AVEC SUCC√àS!")
            logger.info("")
            logger.info("üìù Services disponibles:")
            logger.info("   - API: http://localhost:8000")
            logger.info("   - MLflow: http://localhost:5000")
            logger.info("   - Grafana: http://localhost:3000")
            logger.info("   - Prometheus: http://localhost:9091")
        else:
            logger.error("")
            logger.error("‚ùå PIPELINE √âCHOU√â")
        
        return all_success


def main():
    """Fonction principale."""
    parser = argparse.ArgumentParser(
        description="Pipeline automatis√© MLOps complet"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Forcer l'ex√©cution m√™me sans changements DVC"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Chemin vers le fichier de configuration"
    )
    parser.add_argument(
        "--skip-dvc",
        action="store_true",
        help="Ignorer l'ex√©cution du pipeline DVC"
    )
    parser.add_argument(
        "--skip-docker",
        action="store_true",
        help="Ignorer la construction Docker"
    )
    parser.add_argument(
        "--skip-deploy",
        action="store_true",
        help="Ignorer le d√©ploiement"
    )
    
    args = parser.parse_args()
    
    pipeline = AutomatedPipeline(config_path=args.config, force=args.force)
    
    # Modifier les m√©thodes si n√©cessaire
    if args.skip_dvc:
        pipeline.run_dvc_pipeline = lambda: True
    if args.skip_docker:
        pipeline.build_docker_image = lambda: True
    if args.skip_deploy:
        pipeline.deploy_locally = lambda: True
    
    success = pipeline.run_full_pipeline()
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()

